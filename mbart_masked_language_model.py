# -*- coding: utf-8 -*-
"""mBart-mlm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kfYThgl83CJytVZh4daT6p509lQdhIA5

# Trying Masked Language Modelling with MBart
"""

!pip install git+https://github.com/huggingface/transformers.git@master
!pip install git+https://github.com/huggingface/datasets.git@master
!pip install sentencepiece

from transformers import MBartForConditionalGeneration, MBartTokenizer, MBartForCausalLM
import torch

tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-50-one-to-many-mmt')
model = MBartForCausalLM.from_pretrained('facebook/mbart-large-50-one-to-many-mmt')

with open('/content/mr-500.txt','r') as f:
    text = f.read().split('\n')

inputs = tokenizer(text, return_tensors='pt',padding=True)

inputs.keys()

inputs.input_ids[0]

inputs['labels'] = inputs.input_ids.detach().clone()

rand = torch.rand(inputs.input_ids.shape)
mask_arr = (rand < 0.15) * (inputs.input_ids != 1)
mask_arr

selection = torch.flatten((mask_arr[0]).nonzero()).tolist()
selection

inputs.input_ids[0, selection] = 103
inputs.input_ids[0]

outputs = model(**inputs)

outputs.keys()

outputs.loss

"""# Running Inference on multilingual bart"""

!pip install -q transformers
!pip install -q sentencepiece

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model_name = 'facebook/mbart-large-50-many-to-many-mmt'
model = MBartForConditionalGeneration.from_pretrained(model_name)
tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang = 'mr_IN')



with open('/content/mr-500.txt','r') as f:
    sample_inputs = f.read().split('\n')

sample_inputs[70]

input = tokenizer(sample_inputs[69], return_tensors='pt')
generated_tokens = model.generate(**input, forced_bos_token_id = tokenizer.lang_code_to_id ["bn_IN"])

generated_tokens

trans = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
trans

