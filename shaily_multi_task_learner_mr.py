# -*- coding: utf-8 -*-
"""Shaily - Multi-task-learner-mr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BdtaUmjf1tBnPxIhEZ82Pycbd6gME3Z1

# Setting Up Data
"""

!pip install transformers
!pip install sentencepiece
!pip install datasets
!pip install nlp

import numpy as np
import torch
import torch.nn as nn
from datasets import load_dataset
import datasets
import nlp
import pandas as pd
from transformers import AutoConfig, AutoModelForCausalLM, MBartForConditionalGeneration, MBart50TokenizerFast, MBartForCausalLM
import transformers

from google.colab import drive
drive.mount('/content/drive')

bitext_path = '/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data'
dataset_dict  = {
    'TRANSLATION/mr-en': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Train-mr-en.csv',
                                             'validation':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Val-mr-en.csv',
                                             'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Test-mr-en.csv'}),
    
    'TRANSLATION/mr-bn': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Train-mr-bn.csv',
                                             'validation':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Val-mr-bn.csv',
                                             'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Test-mr-bn.csv'}),

    'TRANSLATION/mr-hi': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Train-mr-hi.csv',
                                             'validation':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Val-mr-hi.csv',
                                             'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Test-mr-hi.csv'}),
                 
    'CLM/mr': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/mr-250k.csv'}),

    'CLM/en': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/en-250k.csv'}),

    'CLM/bn': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/bn-250k.csv'}),

    'CLM/hi': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/hi-250k.csv'})
    
}

dataset_dict

"""# Model

"""

class MultitaskModel(transformers.PreTrainedModel):
    def __init__(self, encoder, taskmodels_dict):
        """
        Setting MultitaskModel up as a PretrainedModel allows us
        to take better advantage of Trainer features
        """
        super().__init__(transformers.PretrainedConfig())

        self.encoder = encoder
        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)

    @classmethod
    def create(cls, model_name, model_type_dict, model_config_dict):
        """
        This creates a MultitaskModel using the model class and config objects
        from single-task models. 

        We do this by creating each single-task model, and having them share
        the same encoder transformer.
        """
        shared_encoder = None
        taskmodels_dict = {}
        for task_name, model_type in model_type_dict.items():
            model = model_type.from_pretrained(
                model_name, 
                config=model_config_dict[task_name],
            )
            if shared_encoder is None:
                shared_encoder = model.get_encoder()
            else:
                setattr(model, 'encoder', shared_encoder)
            taskmodels_dict[task_name] = model
        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)

    @classmethod
    def get_encoder_attr_name(cls, model):
        """
        The encoder transformer is named differently in each model "architecture".
        This method lets us get the name of the encoder attribute
        """
        model_class_name = model.__class__.__name__
        print(model_class_name)
        if model_class_name.startswith("MBartFor"):
            return "encoder"
        else:
            raise KeyError(f"Add support for new model {model_class_name}")

    def forward(self, task_name, **kwargs):
        return self.taskmodels_dict[task_name](**kwargs)

model_name = 'facebook/mbart-large-50'
multitask_model = MultitaskModel.create(
    model_name=model_name,
    model_type_dict={
        'TRANSLATION':MBartForConditionalGeneration,
        'CLM':MBartForCausalLM,
    },
    model_config_dict={
        'TRANSLATION':AutoConfig.from_pretrained(model_name),
        'CLM':AutoConfig.from_pretrained(model_name),
    }
)

multitask_model

print(multitask_model.encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['TRANSLATION'].model.encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['CLM'].encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['CLM'].model.decoder.embed_tokens.weight.data_ptr())

"""# Data"""

tokenizer = MBart50TokenizerFast.from_pretrained(model_name)

def get_features_trans(sample_batch,lang):
    src = sample_batch['text']
    tgt = sample_batch['translation']
    batch = tokenizer.prepare_seq2seq_batch(src_texts=src, src_lang='mr_IN', tgt_texts=tgt, tgt_lang=lang,  
                                            max_length=32, max_target_length=32, padding=True, truncation=True)
    return batch


def get_features_clm(sample_batch,lang):
    tokenizer.src_lang = lang
    examples = tokenizer(sample_batch['text'],max_length=32,truncation=True,padding=True)
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    block_size=32
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

convert_func_dir = {
    'TRANSLATION': get_features_trans,
    'CLM' : get_features_clm
}

columns_dict = {
    "TRANSLATION": ['input_ids', 'attention_mask', 'labels'],
    "CLM": ['attention_mask', 'input_ids', 'labels'],
}

features_dict = {}
for task, dataset in dataset_dict.items():
    task_list = task.split('/')
    task_name = task_list[0]
    lang = task_list[1]

    remove_columns = []
    if task_name == 'TRANSLATION':
        remove_columns=['text', 'translation']
    elif task_name == 'CLM':
        remove_columns=['text']
    if lang == 'mr-en' or lang == 'en':
        lang_name = 'en_XX'
    elif lang == 'mr-bn' or lang == 'bn':
        lang_name = 'bn_IN'
    elif lang_name == 'mr-hi' or lang == 'hi':
        lang_name = 'hi_IN'
    elif lang_name == 'mr':
        lang_name = 'mr_IN'
    features_dict[task] = {}
    for phase, phase_dataset in dataset.items():
        features_dict[task][phase] = phase_dataset.map(
            convert_func_dir[task_name],
            batched=True,
            load_from_cache_file=False,
            fn_kwargs = {'lang':lang_name},
            remove_columns=remove_columns
        )
        print(task_name, phase, len(phase_dataset), len(features_dict[task][phase]))
        features_dict[task][phase].set_format(
            type="torch", 
            columns=columns_dict[task_name],
        )
        print(task_name, phase, len(phase_dataset), len(features_dict[task][phase]))

dataset_dict['mr-en'].items()

type(multitask_model)

multitask_model.load_state_dict