# -*- coding: utf-8 -*-
"""(Multimodal) RobertaModel for regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Afvuc19ZndjoTADBg14kUH27-iBqRbc
"""

!pip install spacy nltk transformers

from google.colab import drive
drive.mount("/content/drive/")

import torch
from torch.utils.data import DataLoader, SubsetRandomSampler, Subset
from torch.optim.lr_scheduler import ReduceLROnPlateau
import pandas as pd
import numpy as np
import re
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.corpus import words, wordnet, brown
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import unicodedata
import transformers
from transformers import BertTokenizer, BertTokenizer
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import AdamW, get_linear_schedule_with_warmup, Trainer, TrainingArguments
from transformers import set_seed
import random
import time
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('words')
nltk.download('brown')
from ast import literal_eval

set_seed(73)

class EmpathyDistressDataset(torch.utils.data.Dataset):
    """Parameters:
       data_path:
       tokenizer: Pass an initialized tokenizer object
       max_len: Compute max length prior
       attrs: pass a list of training variables(the exact column names in the dataframe
       label: pass the column name of the target variable
       empathy_lex and distress_lex: path to csv file of lexicons"""

    def __init__(self,data_path,tokenizer,max_len,attrs,label):
        #dataset
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.attrs = attrs
        self.label = label

        #preprocessing
        self.stop_words = stopwords.words('english')
        self.nlp = spacy.load('en_core_web_sm')

        #filetypes
        if self.data_path.endswith('tsv'):
            self.dataframe = pd.read_csv(self.data_path,delimiter='\t')
        elif self.data_path.endswith('csv'):
            self.dataframe = pd.read_csv(self.data_path)
        else:
            raise TypeError("Wrong filetype")

    def preprocess(self, text):
        text = re.sub(r"([\w/'+$\s-]+|[^\w/'+$\s-]+)\s*", r"\1 ", text)
        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')  
        text = re.sub(r'^\s*|\s\s*', ' ', text).strip()
        text = re.sub('\S*@\S*\s?', '', text)
        text = re.sub('\s+', ' ', text) 
        text = re.sub("\'", '', text)
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = re.sub('\s+[a-zA-Z]\s+^I', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.lower()
        text = word_tokenize(text)
        text = " ".join([word for word in text if word not in self.stop_words])
     
        return text

    # def lex_transform(self, essay, label):
    #     if label=='distress':
    #         df = self.distress_dataframe
    #     elif label=='empathy':
    #         df = self.empathy_dataframe
    #     temp_arr = []
    #     for word in essay.split():
    #         if word in df['word'].tolist():
    #             temp_arr.append(round(df.loc[df['word'] == word, 'rating'].iloc[0],2))
    #     lex_array = np.array(temp_arr)
    #     lex_array.resize(60)
    #     return lex_array

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self,idx):
        item = {}
        feature_array = []
        for attr in self.attrs:
            if attr=="essay":
                temp = self.dataframe.loc[idx,attr]
                temp = self.preprocess(temp)
                input_dict = self.tokenizer.encode_plus(temp, 
                                                   max_length = self.max_len, 
                                                   truncation=True, 
                                                   return_tensors="pt",
                                                   padding='max_length')
                item['input_ids'] = input_dict['input_ids']
                item['attention_mask'] = input_dict['attention_mask']
                
            elif attr=='emotion':
                temp = self.dataframe[attr].str.get_dummies().values.tolist()[idx]
                item[attr] = temp
            else:
                feature_array.append(self.dataframe.loc[idx, attr])
        item["feature_array"] = feature_array
        item["label"] = torch.Tensor([self.dataframe.loc[idx,self.label]])

        if self.label=="empathy" or self.label=='distress':
            encoding = np.array(literal_eval(self.dataframe.loc[idx, "distress_tokenized"]))
            item["lex_encoding"] = torch.Tensor(encoding)

        return item

pd.read_csv('/content/UPDATED_DATASET_FIXED.csv')[:25]

def make_dataset():
    dataset = EmpathyDistressDataset("/content/UPDATED_DATASET_FIXED.csv",
                                     tokenizer,
                                     max_len=100,
                                     attrs=["essay",
                                            "gender",
                                            "education",
                                            "age",
                                            "iri_perspective_taking",
                                            "iri_personal_distress",
                                            "iri_fantasy",
                                            "iri_empathatic_concern"],
                                     label="distress")
    batch_size = 32
    validation_split = 0.2
    shuffle_dataset = False
    random_seed= 42

    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    split = int(np.floor(validation_split * dataset_size))
    if shuffle_dataset :
        np.random.seed(random_seed)
        np.random.shuffle(indices)
    train_indices, val_indices = indices[split:], indices[:split]

    train_dataset = Subset(dataset,train_indices)
    val_dataset = Subset(dataset,val_indices)
    return train_dataset, val_dataset

tokenizer = AutoTokenizer.from_pretrained('roberta-base')
train_dataset, val_dataset = make_dataset()

class RobertaRegressor(torch.nn.Module):
    def __init__(self, freeze_roberta=False):
        super(RobertaRegressor, self).__init__()

        input_shape = 768
        hidden_shape = 64
        output_shape = 1
        self.config = AutoConfig.from_pretrained("roberta-base", output_hidden_states=False)
        self.roberta = AutoModel.from_pretrained("roberta-base", config=self.config)

        # self.attributes_encoder = torch.nn.Sequential(
        #     torch.nn.Linear(8, 16), 
        #     torch.nn.ReLU(),
        #     torch.nn.Linear(16, 4), 
        #     torch.nn.ReLU()
        # )

        self.roberta_head = torch.nn.Sequential(
            torch.nn.Linear(768,128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU()
        )

        self.lex_head = torch.nn.Sequential(
            torch.nn.Linear(60,16),
            # torch.nn.ReLU(),
            # torch.nn.Linear(16, 4),
            torch.nn.ReLU()
        )

        self.regressor = torch.nn.Sequential(
            torch.nn.Linear(87, 32), 
            torch.nn.ReLU(),
            torch.nn.Linear(32, 16),
            torch.nn.ReLU(),
            torch.nn.Linear(16, 1),
        )


        if freeze_roberta:
            for param in self.roberta.parameters():
                param.requires_grad = False
    

    def forward(self, input_ids, attention_mask, feature_array, lex_array,
                decoder_input_ids=True, decoder_attention_mask=True, testing=False):
        
        outputs = self.roberta(input_ids=input_ids, 
                               attention_mask=attention_mask)
        roberta_outputs = self.roberta_head(outputs[0][:,0,:])
    
        #attr_features = self.attributes_encoder(feature_array.float())
        attr_features = feature_array
        lex_features = self.lex_head(lex_array)
        #print(roberta_outputs.shape, attr_features.shape, lex_features.shape)
        comb_features = torch.cat((roberta_outputs, attr_features.float(), lex_features), dim=1)

        logits = self.regressor(comb_features)
        return logits

class RobertaTrainer(transformers.Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.model=model

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs['labels']
        labels = labels.unsqueeze(1)

        logits = model.forward(input_ids=inputs['input_ids'].squeeze(),
                               attention_mask=inputs['attention_mask'].squeeze(),
                               feature_array=inputs['feature_array'],
                               lex_array=inputs['lex_encoding']
                               )
        
        # print("--LOGITS--")
        # print(logits)
        # print("--LABELS--")
        # print(labels)

        loss_fct = torch.nn.MSELoss()

        # print("Logits:",logits.shape,logits)
        # print("Labels:",labels.shape,labels)
        loss = loss_fct(logits.squeeze(), labels.squeeze())

        return (loss, logits) if return_outputs else loss

    def create_optimizer_and_scheduler(self,num_training_steps=1800):
        self.optimizer = AdamW(self.model.parameters(),
                      lr=0.001
                      )
        self.lr_scheduler = get_linear_schedule_with_warmup(self.optimizer,
                                                           num_warmup_steps=1,
                                                            num_training_steps=1800)

model = RobertaRegressor(freeze_roberta=True)
#model.to('cuda')
training_args = TrainingArguments(
    output_dir='/content/Roberta-distress-test-iri/frozen',       
    num_train_epochs=150,             
    per_device_train_batch_size=128,  
    per_device_eval_batch_size=128,
    do_eval=True,
    evaluation_strategy='epoch',
    save_steps = 1800,
    logging_strategy='epoch'
)
trainer = RobertaTrainer(
    model=model,                         
    args=training_args,                 
    train_dataset=train_dataset,      
    eval_dataset=val_dataset,     
)
trainer.create_optimizer_and_scheduler()
#trainer.create_optimizer()
trainer.train()

validation_loader = DataLoader(val_dataset,batch_size=64)
labels = []
gold = []

model.eval()
with torch.no_grad():
    for i,input in enumerate(validation_loader):
        label = model.forward(input['input_ids'].squeeze().to('cuda'),
                              input['attention_mask'].squeeze().to('cuda'),
                              feature_array=torch.stack(input['feature_array'],1).to('cuda'),
                              lex_array=input['lex_encoding'].to('cuda'),
                              testing=True)
        if i%5==0:
            print(label)
        gold.append(input['label'])
        labels.append(label)

actual = []
for item in gold:
    for i in item:
        actual.append(i.item())

pred = []
for item in labels:
    for i in item:
        pred.append(i.detach().cpu().item())

len(pred)

from scipy.stats import pearsonr
corr, _ = pearsonr(pred,actual)
print('Pearsons correlation: %.3f' % corr)

# def pearson(X,Y):
#     xm = torch.mean(X)
#     ym = torch.mean(Y)
#     num = 0.0
#     deno1 = 0.0
#     deno2 = 0.0
#     deno = 0.0
#     n = X.shape[0]
#     for i in range(n):
#         num += (X[i]-xm)*(Y[i]-ym)
#         deno1 += (X[i]-xm)*(X[i]-xm)
#         deno2 += (Y[i]-ym)*(Y[i]-ym)
#     deno = deno1*deno2
#     deno = torch.sqrt(deno)
#     return (num/deno).item()

# corr = pearson(torch.Tensor(pred),torch.Tensor(actual))
# print('Pearsons correlation: %.3f' % corr)

torch.save(model.state_dict(), "/content/drive/MyDrive/WASSA 2022/roberta_distress_multinput_433_with_iri.pt")

