# -*- coding: utf-8 -*-
"""Copy of MTL-Working-Edition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CIVZh0OCrJ5mwaC1xoFd_DkpiL2ITpA
"""

!pip install transformers
!pip install sentencepiece
!pip install datasets

import numpy as np
import torch
import torch.nn as nn
from datasets import load_dataset, concatenate_datasets
import datasets
import pandas as pd
from transformers import (AutoConfig, AutoModelForCausalLM,
                          MBartForConditionalGeneration, MBart50TokenizerFast, 
                          MBartForCausalLM, AutoTokenizer, 
                          AutoModelForSeq2SeqLM, AutoModel)
import transformers
from transformers.data.data_collator import default_data_collator

from google.colab import drive
drive.mount('/content/drive')

dataset_dict  = {
    'TRANSLATION/en-mr': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bilingual/Train-EN-MR.csv',
                                            'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bilingual/Test-EN-MR.csv'}),
    
    #'TRANSLATION/mr-bn': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Train-mr-bn.csv',
    #                                         'validation':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Val-mr-bn.csv',
    #                                        'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Test-mr-bn.csv'}),

    #'TRANSLATION/mr-hi': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Train-mr-hi.csv',
    #                                        'validation':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Val-mr-hi.csv',
    #                                         'test':'/content/drive/MyDrive/Marath-Multitask-NMT/Bitext_data/Test-mr-hi.csv'}),
                 
    'CLM/mr': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/mr-70k.csv'}),

    'CLM/en': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/en-70k.csv'}),

   # 'CLM/bn': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/bn-70k.csv'}),

    #'CLM/hi': load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Marath-Multitask-NMT/mlm_data/hi-70k.csv'})
    
}

class MultitaskModel(transformers.PreTrainedModel):
    def __init__(self, encoder, taskmodels_dict):
        """
        Setting MultitaskModel up as a PretrainedModel allows us
        to take better advantage of Trainer features
        """
        super().__init__(transformers.PretrainedConfig())

        self.encoder = encoder
        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)

    @classmethod
    def create(cls, model_name, model_type_dict, model_config_dict):
        """
        This creates a MultitaskModel using the model class and config objects
        from single-task models. 

        We do this by creating each single-task model, and having them share
        the same encoder transformer.
        """
        shared_encoder = None
        taskmodels_dict = {}
        for task_name, model_type in model_type_dict.items():
            model = model_type.from_pretrained(
                model_name, 
                config=model_config_dict[task_name],
            )
            if shared_encoder is None:
                shared_encoder = model.get_encoder()
            else:
                setattr(model, 'encoder', shared_encoder)
            taskmodels_dict[task_name] = model
        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)

    @classmethod
    def get_encoder_attr_name(cls, model):
        """
        The encoder transformer is named differently in each model "architecture".
        This method lets us get the name of the encoder attribute
        """
        model_class_name = model.__class__.__name__
        print(model_class_name)
        if model_class_name.startswith("MBartFor"):
            return "encoder"
        else:
            raise KeyError(f"Add support for new model {model_class_name}")

    def forward(self,task_name,**kwargs):
        return self.taskmodels_dict[task_name](**kwargs)
    
    def generate(self, task_name, **kwargs):
        return self.taskmodels_dict[task_name].generate(**kwargs)

model_name = 'facebook/mbart-large-50'
multitask_model = MultitaskModel.create(
    model_name=model_name,
    model_type_dict={
        'TRANSLATION':MBartForConditionalGeneration,
        'CLM':MBartForCausalLM,
    },
    model_config_dict={
        'TRANSLATION':AutoConfig.from_pretrained(model_name),
        'CLM':AutoConfig.from_pretrained(model_name),
    }
)

print(multitask_model.encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['TRANSLATION'].model.encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['CLM'].encoder.embed_tokens.weight.data_ptr())
print(multitask_model.taskmodels_dict['CLM'].model.decoder.embed_tokens.weight.data_ptr())

modules = [*multitask_model.encoder.layers[:5], multitask_model.encoder.embed_tokens] #Replace 5 by what you want
for module in modules:
    for param in module.parameters():
        param.requires_grad = False

tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')

def get_features_trans(sample_batch,lang):
    src = sample_batch['text']
    tgt = sample_batch['translation']
    batch = tokenizer.prepare_seq2seq_batch(src_texts=src, src_lang='en_XX', tgt_texts=tgt, tgt_lang='mr_IN',  
                                            max_length=32, max_target_length=16, padding=True, truncation=True)
    return batch


def get_features_clm(sample_batch,lang):
    tokenizer.src_lang = lang
    examples = tokenizer(sample_batch['text'],max_length=32,truncation=True,padding=True)
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    block_size=32
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

convert_func_dir = {
    'TRANSLATION': get_features_trans,
    'CLM' : get_features_clm
}

columns_dict = {
    "TRANSLATION": ['input_ids', 'attention_mask', 'labels'],
    "CLM": ['attention_mask', 'input_ids', 'labels'],
}

features_dict = {}
for task, dataset in dataset_dict.items():
    task_list = task.split('/')
    task_name = task_list[0]
    lang = task_list[1]

    lang_name = ''
    remove_columns = []
    if task_name == 'TRANSLATION':
        remove_columns=['text', 'translation']
    elif task_name == 'CLM':
        remove_columns=['text']
    if lang == 'mr-en' or lang == 'en':
        lang_name = 'en_XX'
    elif lang == 'mr-bn' or lang == 'bn':
        lang_name = 'bn_IN'
    elif lang == 'mr-hi' or lang == 'hi':
        lang_name = 'hi_IN'
    elif lang == 'mr' or lang == 'en-mr':
        lang_name = 'mr_IN'
    features_dict[task] = {}
    for phase, phase_dataset in dataset.items():
        features_dict[task][phase] = phase_dataset.map(
            convert_func_dir[task_name],
            batched=True,
            load_from_cache_file=False,
            fn_kwargs = {'lang':lang_name},
            remove_columns=remove_columns
        )
        print(task_name, phase, len(phase_dataset), len(features_dict[task][phase]))
        features_dict[task][phase].set_format(
            type="torch", 
            columns=columns_dict[task_name],
        )
        print(task_name, phase, len(phase_dataset), len(features_dict[task][phase]))

clm_data = [features_dict['CLM/mr']['train'], features_dict['CLM/en']['train']]
trans_data = [features_dict['TRANSLATION/en-mr']['train']]

CLM_DATASET = concatenate_datasets(clm_data)
TRANS_DATASET = concatenate_datasets(trans_data)

import dataclasses
from torch.utils.data.dataloader import DataLoader
from transformers.data.data_collator import DataCollator, InputDataClass
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data.sampler import RandomSampler
from typing import List, Union, Dict

# class NLPDataCollator(): 

#     def __call__(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:
#         collate_batch(features)
#         #     first = features[0]
#         #     if isinstance(first, dict):
#         #       # NLP data sets current works presents features as lists of dictionary
#         #       # (one per example), so we  will adapt the collate_batch logic for that
#         #       if "labels" in first and first["labels"] is not None:
#         #           if first["labels"].dtype == torch.int64:
#         #               labels = torch.tensor([f["labels"] for f in features], dtype=torch.long)
#         #           else:
#         #               labels = torch.tensor([f["labels"] for f in features], dtype=torch.float)
#         #           batch = {"labels": labels}
#         #       for k, v in first.items():
#         #           if k != "labels" and v is not None and not isinstance(v, str):
#         #               batch[k] = torch.stack([f[k] for f in features])
#         #       return batch
#         #     else:
#         #       # otherwise, revert to using the default collate_batch
#             return DefaultDataCollator().collate_batch(features)

class StrIgnoreDevice(str):
    def to(self, device):
        return self


class DataLoaderWithTaskname:
    def __init__(self, task_name, data_loader):
        self.task_name = task_name
        self.data_loader = data_loader

        self.batch_size = data_loader.batch_size
        self.dataset = data_loader.dataset

    def __len__(self):
        return len(self.data_loader)
    
    def __iter__(self):
        for batch in self.data_loader:
            batch["task_name"] = StrIgnoreDevice(self.task_name)
            yield batch


class MultitaskDataloader:
    def __init__(self, dataloader_dict):
        self.dataloader_dict = dataloader_dict
        self.num_batches_dict = {
            task_name: len(dataloader) 
            for task_name, dataloader in self.dataloader_dict.items()
        }
        self.task_name_list = list(self.dataloader_dict)
        self.dataset = [None] * sum(
            len(dataloader.dataset) 
            for dataloader in self.dataloader_dict.values()
        )

    def __len__(self):
        return sum(self.num_batches_dict.values())

    def __iter__(self):
        task_choice_list = []
        for i, task_name in enumerate(self.task_name_list):
            task_choice_list += [i] * self.num_batches_dict[task_name]
        task_choice_list = np.array(task_choice_list)
        np.random.shuffle(task_choice_list)
        dataloader_iter_dict = {
            task_name: iter(dataloader) 
            for task_name, dataloader in self.dataloader_dict.items()
        }
        for task_choice in task_choice_list:
            task_name = self.task_name_list[task_choice]
            yield next(dataloader_iter_dict[task_name])    

class MultitaskTrainer(transformers.Trainer):

    def get_single_train_dataloader(self, task_name, train_dataset):
        if self.train_dataset is None:
            raise ValueError("Trainer: training requires a train_dataset.")
        else:
            train_sampler = (
                RandomSampler(train_dataset)
                if self.args.local_rank == -1
                else DistributedSampler(train_dataset)
            )

        data_loader = DataLoaderWithTaskname(
            task_name=task_name,
            data_loader=DataLoader(
              train_dataset,
              batch_size=self.args.train_batch_size,
              sampler=train_sampler,
              collate_fn=default_data_collator,
            ),
        )

        return data_loader

    def get_train_dataloader(self):
        return MultitaskDataloader({
            task_name: self.get_single_train_dataloader(task_name, task_dataset)
            for task_name, task_dataset in self.train_dataset.items()
        })

train_dataset = {
    'TRANSLATION' : TRANS_DATASET,
    'CLM' : CLM_DATASET
}

trainer = MultitaskTrainer(
    model=multitask_model,
    args=transformers.TrainingArguments(
        output_dir="/content/bilingual_model_en_mr",
        overwrite_output_dir=True,
        learning_rate=1e-5,
        do_train=True,
        do_eval=False,
        num_train_epochs=1,
        # Adjust batch size if this doesn't fit on the Colab GPU
        per_device_train_batch_size=2,  
        save_steps=10000,
    ),
    data_collator=default_data_collator,
    train_dataset=train_dataset,
)

trainer.train()

test_dataset = load_dataset('csv',data_files='/content/drive/MyDrive/Marath-Multitask-NMT/Bilingual/Test-EN-MR.csv')

def get_preds(lang_text,lang):    
    preds = []
    i = 0
    for text in lang_text:
        multitask_model.eval()
        strip = text.strip()
        source = tokenizer.encode(strip,max_length=32)
        generated = multitask_model.generate('TRANSLATION',**{'input_ids':torch.LongTensor([source]).to('cuda'),
                                          'forced_bos_token_id':tokenizer.lang_code_to_id['mr_IN']})
        
        translation = tokenizer.decode(generated[0],skip_special_tokens=True,clean_up_tokenization_spaces=True)
        
        if i%10 == 0:
            print(f'{i} iterations DONE')
            print(text, translation)
        i+=1
        preds.append(translation)
    return preds

en_preds = get_preds(test_dataset['train']['text'],lang='mr_IN')

import nltk
from nltk.translate.bleu_score import SmoothingFunction

def get_bleu_score(trans,preds,weights):
    smoothie = SmoothingFunction().method4

    list_of_references = [[trans[i].split()] for i in range(len(trans))]
    list_of_hypotheses = [preds[i].split() for i in range(len(preds))]

    score = nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses,smoothing_function=smoothie,weights=weights)
    return score

print(f"BLEU for English - Marathi: {get_bleu_score(test_dataset['train']['translation'][:40],en_preds[:40],(0.25,0.25,0.25,0.25))}")

